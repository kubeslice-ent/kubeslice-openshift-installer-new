---
- name: Installation playbook
  hosts: localhost
  connection: local
  #become: true   #tasks as root user
  vars_files:
    - vars/install-vars.yaml


  tasks:

  - debug:
      msg:     
      - '###################################################################'
      - '#              Deleting serviceexport                             #'
      - '###################################################################'

  - tags: mongodb
    shell: |
      kubectx | awk 'FNR == 1'
    register: master_cluster
    environment:
      KUBECONFIG: files/kubeconfig/merged.config
  - tags: mongodb
    shell: |
      kubectx | awk 'FNR == 1'
    register: first_cluster
    environment:
      KUBECONFIG: files/kubeconfig/merged.config
  - tags: mongodb
    shell: |
      kubectx | awk 'FNR == 2'
    register: second_cluster
    environment:
      KUBECONFIG: files/kubeconfig/merged.config
  - tags: mongodb
    shell: |
      kubectx | awk 'FNR == 3'
    register: third_cluster
    environment:
      KUBECONFIG: files/kubeconfig/merged.config

  - name: List of context in kubeconfig
    tags: mongodb
    debug:
      msg: 
      - "{{ master_cluster.stdout }}"  
      - "{{ first_cluster.stdout }}" 
      - "{{ second_cluster.stdout }}" 
      - "{{ third_cluster.stdout }}"
      
  - name: deleteing ServiceExport for each mongodb member cluster
    no_log: true
    ignore_errors: yes
    tags:
    - mongodb
    shell: |
      kubectl --kubeconfig="{{ clusters.worker.0.kubeconfig }}" delete -f files/mongodb/serviceexport/multi-replica-set-0.yaml
      kubectl --kubeconfig="{{ clusters.worker.1.kubeconfig }}" delete -f files/mongodb/serviceexport/multi-replica-set-1.yaml
      kubectl --kubeconfig="{{ clusters.worker.2.kubeconfig }}" delete -f files/mongodb/serviceexport/multi-replica-set-2.yaml

  - debug:
      msg:     
      - '###################################################################'
      - '#              Deleting MongoDB replica set                       #'
      - '###################################################################'

  - name: deleting MongoDB replica set using the MongoDBMultiCRD
    ignore_errors: yes
    tags:
    - mongodb
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    kubernetes.core.k8s:
      state: absent
      namespace: "{{ item.0.namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
      definition: |
        apiVersion: mongodb.com/v1
        kind: MongoDBMulti
        metadata:
          name: multi-replica-set
        spec:
          version: "{{ item.0.MDB_version }}"
          type: ReplicaSet
          persistent: true 
          duplicateServiceObjects: true 
          credentials: multi-organization-secret 
          opsManager:
            configMapRef:
              name: multi-project 
          clusterSpecList:
            clusterSpecs:
            - clusterName: {{ first_cluster.stdout }}
              members: 1
            - clusterName: {{ second_cluster.stdout }}
              members: 1
            - clusterName: {{ third_cluster.stdout }}  
              members: 1

### Deleting configmap when nodeport
  - tags:
    - mongodb
    no_log: false
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    - item.0.Ops_manager_service_type == "NodePort"
    register: mongodb 
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Service
      name: ops-manager-svc-ext
      namespace: "{{ item.0.operator_namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
  - no_log: true
    tags: 
    - mongodb
    ignore_errors: yes
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    - item.0.Ops_manager_service_type == "NodePort"
    shell: kubectl --kubeconfig="{{ item.1.kubeconfig }}" get nodes -o wide | grep -i Ready |  awk 'FNR == 1 {print$7}' 
    register: nodeip
  - name: deleting configmap with Ops-manager values
    ignore_errors: yes
    no_log: true
    tags:
    - mongodb
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    - "{{ mongodb.results }}"
    - "{{ nodeip.results }}"
    loop_control:
      label: 'mongodb'
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    - item.0.Ops_manager_service_type == "NodePort"
    - item.0.workerName   ==  item.2.item.0.workerName 
    - item.0.workerName  ==   item.2.item.1.name 
    - item.0.workerName  ==   item.3.item.1.name 
    - item.0.workerName  ==   item.3.item.0.workerName 
    kubernetes.core.k8s:
      state: absent
      namespace: "{{ item.0.namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
      definition: |     
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: multi-project
          namespace: {{ item.0.namespace }}
        data:
          baseUrl: http://{{ item.1.nodeIp | default(item.3.stdout, true) }}:{{ item.2.resources.0.spec.ports.0.nodePort }}
          orgId: {{ item.0.orgId }}

### Deleting configmap when loadbalancer
  - no_log: true
    tags: 
    - mongodb
    # ignore_errors: yes
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    - item.0.Ops_manager_service_type == "LoadBalancer"
    shell: kubectl --kubeconfig="{{ item.1.kubeconfig }}" -n {{ item.0.operator_namespace }} get svc ops-manager-svc-ext -o jsonpath='{.status.loadBalancer.ingress[0].ip}:{.spec.ports[0].port}'
    register: mongodb
  - name: deleting configmap with Ops-manager values
    tags: 
    - mongodb
    ignore_errors: yes
    no_log: true
    loop: "{{ mongodb.results }}"
    loop_control:
      label: 'mongodb'
    when: 
    - item.changed == true 
    - item.item.0.mongodb_master == true 
    - item.item.0.Ops_manager_service_type == "LoadBalancer"
    kubernetes.core.k8s:
      state: absent
      namespace: "{{ item.0.namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
      definition: |     
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: multi-project
          namespace: {{ item.0.namespace }}
        data:
          baseUrl: http://{{ item.stdout }}
          orgId: {{ item.0.orgId }}

  - name: deleting secret with Ops-manager key's
    ignore_errors: yes
    tags:
    - mongodb
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    kubernetes.core.k8s:
      state: absent
      namespace: "{{ item.0.namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
      definition: |     
        apiVersion: v1
        kind: Secret
        metadata:
          name: multi-organization-secret
          namespace: {{ item.0.namespace }}
        stringData:
          publicKey: {{ item.0.publicKey }}
          privateKey: {{ item.0.privateKey }}


## deleting the required service accounts for each member cluster.
  - name: deleting required service accounts for each member cluster
    ignore_errors: yes
    no_log: true
    tags: 
    - mongodb
    shell: |
      helm --kube-context {{ master_cluster.stdout }} template --show-only templates/database-roles.yaml mongodb/enterprise-operator --namespace "mongodb" | kubectl delete -f - --context={{ first_cluster.stdout }} --namespace mongodb;
      helm --kube-context {{ master_cluster.stdout }} template --show-only templates/database-roles.yaml mongodb/enterprise-operator --namespace "mongodb" | kubectl delete -f - --context={{ second_cluster.stdout }} --namespace mongodb;
      helm --kube-context {{ master_cluster.stdout }} template --show-only templates/database-roles.yaml mongodb/enterprise-operator --namespace "mongodb" | kubectl delete -f - --context={{ third_cluster.stdout }} --namespace mongodb;
    register: helm_template
    environment:
      KUBECONFIG: files/kubeconfig/merged.config  

  - debug:
      msg:     
      - '###################################################################'
      - '#              uninstalling Ops manager on worker                 #'
      - '###################################################################'

  - name: Uninstalling Ops Manager 
    ignore_errors: yes
    tags:
    - mongodb
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    kubernetes.core.k8s:
      state: absent
      namespace: "{{ item.0.operator_namespace }}"
      kubeconfig: "{{ item.1.kubeconfig }}"
      validate_certs: "{{ validate_certs }}"
      definition: |     
        apiVersion: mongodb.com/v1
        kind: MongoDBOpsManager
        metadata:
          name: ops-manager
        spec:
          version: 6.0.5
          # the name of the secret containing admin user credentials.
          adminCredentials: om-admin-secret
          externalConnectivity:
            type: {{ item.0.Ops_manager_service_type }}
          configuration:
            mms.ignoreInitialUiSetup: "true"
            automation.versions.source: mongodb
            mms.adminEmailAddr: {{ item.0.username }}
            mms.fromEmailAddr: {{ item.0.username }}
            mms.replyToEmailAddr: {{ item.0.username }}
            mms.mail.hostname: aveshasystems.com
            mms.mail.port: "465"
            mms.mail.ssl: "false"
            mms.mail.transport: smtp
          # the Replica Set backing Ops Manager.
          applicationDatabase:
            members: 3
            version: 5.0.5-ent

  - name: deleting secret for MongoDB Ops Manager
    ignore_errors: yes
    no_log: true
    tags:
    - mongodb
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    shell: |
      kubectl --kubeconfig="{{ item.1.kubeconfig }}" -n {{ item.0.operator_namespace }}  delete secret om-admin-secret 

  - debug:
      msg:     
      - '###################################################################'
      - '#              uninstalling mongodb operator on worker           #'
      - '###################################################################'

  - name: uninstalling mongodb operator on worker
    tags:
    - mongodb
    ignore_errors: yes
    with_nested:
    - "{{ mongodb_cluster }}"
    - "{{ clusters.worker }}"
    when: 
    - item.0.workerName  ==  item.1.name 
    - item.0.mongodb_master == true 
    kubernetes.core.helm:
      state: absent
      name: mongodb-enterprise-operator-multi-cluster
      chart_ref: "{{ mongodb_helm_repo_name }}/enterprise-operator"
      chart_version: "{{ item.0.chart_version }}"
      release_namespace: "{{ item.0.namespace }}"
      create_namespace: true  
      kubeconfig: "{{ item.1.kubeconfig }}"
      values:
        namespace: "{{ item.0.namespace }}"
        multiCluster: 
          clusters: "{{ first_cluster.stdout }},{{ second_cluster.stdout }},{{ third_cluster.stdout }}"  
          performFailover: false 
        operator:
          name: mongodb-enterprise-operator-multi-cluster
          createOperatorServiceAccount: false
