---
# tasks file for kafka-zookeeper-setup

- name: Create a namespace for kafka setup application deployment
  tags:
  - kafka
  loop:
     - "{{ clusters.worker.0.kubeconfig }}"
     - "{{ clusters.worker.1.kubeconfig }}"
     - "{{ clusters.worker.2.kubeconfig }}"
  kubernetes.core.k8s:
    name: kafka
    wait_sleep: 5
    api_version: v1
    kind: Namespace
    state: present
    kubeconfig: "{{ item }}"
    validate_certs: "{{ validate_certs }}"
 
#openshift
- name: giving privillage to serviceaccount of kafka
  when : 
  - clusters.worker.0.openshift_cluster == true
  - clusters.worker.1.openshift_cluster == true
  - clusters.worker.2.openshift_cluster == true
  tags:
  - kafka
  loop:
     - "{{ clusters.worker.0.kubeconfig }}"
     - "{{ clusters.worker.1.kubeconfig }}"
     - "{{ clusters.worker.2.kubeconfig }}"
  shell: "oc adm policy add-scc-to-group privileged system:serviceaccounts:kafka --kubeconfig={{ item }}"

#openshift
- name: giving privillage to serviceaccount of kafka
  when : 
  - clusters.worker.0.openshift_cluster == true
  - clusters.worker.1.openshift_cluster == true
  - clusters.worker.2.openshift_cluster == true
  tags:
  - kafka
  loop:
     - "{{ clusters.worker.0.kubeconfig }}"
     - "{{ clusters.worker.1.kubeconfig }}"
     - "{{ clusters.worker.2.kubeconfig }}"
  shell: "kubectl --kubeconfig={{ item }} label --overwrite ns kafka pod-security.kubernetes.io/enforce=privileged"


- name: wait
  ansible.builtin.pause:
    seconds: "{{ chart_deploy_timeout }}"

########################################################################
#################   Zookeeper Installation      ########################
#######################################################################

- name: installing zookeeper on worker 1
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.0.kubeconfig }}"
    # src: files/kafka/kafka-cluster-1.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: zookeeper-0
      spec:
        replicas: 1
        serviceName: zookeeper-cluster
        selector:
          matchLabels:
            app: zookeeper-cluster
        template:
          metadata:
            labels:
              component: zookeeper-cluster
              app: zookeeper-cluster
          spec:
            containers:
            - name: zookeeper
              image: bitnami/zookeeper:latest
              securityContext:
                runAsUser: 0 
              ports:
              # - containerPort: 2181
              # - containerPort: 2888
              # - containerPort: 3888
              - containerPort: 2181
                name: client
              - containerPort: 2888
                name: follower
              - containerPort: 3888
                name: leader
              lifecycle:
                postStart:
                  exec:
                    command:
                      - "sh"
                      - "-c"
                      - >
                        echo $(( $(cat /etc/hosts | grep zookeeper | awk '{print($3)}' | awk '{split($0,array,"-")} END{print array[3]}') + 1 )) > /bitnami/zookeeper/data/myid
              env:
              - name: ZOO_SERVER_ID
                value: "1"
              - name: ALLOW_ANONYMOUS_LOGIN
                value: "yes"
              - name: ZOO_SERVERS
                value: 0.0.0.0:2888:3888,zookeeper-1.kafka.svc.slice.local:2888:3888,zookeeper-2.kafka.svc.slice.local:2888:3888
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: zookeeper-cluster
      spec:
        selector:
          app: zookeeper-cluster
        ports:
        - name: client
          port: 2181
          targetPort: 2181 
        - name: follower
          port: 2888
          targetPort: 2888 
        - name: leader
          port: 3888
          targetPort: 3888
        clusterIP: None

- name: installing zookeeper on worker 2
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.1.kubeconfig }}"
    # src: files/kafka/kafka-cluster-2.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: zookeeper-1
      spec:
        replicas: 1
        serviceName: zookeeper-cluster
        selector:
          matchLabels:
            app: zookeeper-cluster
        template:
          metadata:
            labels:
              component: zookeeper-cluster
              app: zookeeper-cluster
          spec:
            containers:
            - name: zookeeper
              image: bitnami/zookeeper:latest
              securityContext:
                runAsUser: 0 
              ports:
              # - containerPort: 2181
              # - containerPort: 2888
              # - containerPort: 3888
              - containerPort: 2181
                name: client
              - containerPort: 2888
                name: follower
              - containerPort: 3888
                name: leader
              lifecycle:
                postStart:
                  exec:
                    command:
                      - "sh"
                      - "-c"
                      - >
                        echo $(( $(cat /etc/hosts | grep zookeeper | awk '{print($3)}' | awk '{split($0,array,"-")} END{print array[3]}') + 2 )) > /bitnami/zookeeper/data/myid
              env:
              - name: ZOO_SERVER_ID
                value: "2 "
              - name: ALLOW_ANONYMOUS_LOGIN
                value: "yes"
              - name: ZOO_SERVERS
                value: zookeeper-0.kafka.svc.slice.local:2888:3888,0.0.0.0:2888:3888,zookeeper-2.kafka.svc.slice.local:2888:3888
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: zookeeper-cluster
      spec:
        selector:
          app: zookeeper-cluster
        ports:
        - name: client
          port: 2181
          targetPort: 2181 
        - name: follower
          port: 2888
          targetPort: 2888 
        - name: leader
          port: 3888
          targetPort: 3888
        clusterIP: None

- name: installing zookeeper on worker 3
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.2.kubeconfig }}"
    # src: files/kafka/kafka-cluster-3.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: zookeeper-2
      spec:
        replicas: 1
        serviceName: zookeeper-cluster
        selector:
          matchLabels:
            app: zookeeper-cluster
        template:
          metadata:
            labels:
              component: zookeeper-cluster
              app: zookeeper-cluster
          spec:
            containers:
            - name: zookeeper
              image: bitnami/zookeeper:latest
              securityContext:
                runAsUser: 0 
              ports:
              # - containerPort: 2181
              # - containerPort: 2888
              # - containerPort: 3888
              - containerPort: 2181
                name: client
              - containerPort: 2888
                name: follower
              - containerPort: 3888
                name: leader
              lifecycle:
                postStart:
                  exec:
                    command:
                      - "sh"
                      - "-c"
                      - >
                        echo $(( $(cat /etc/hosts | grep zookeeper | awk '{print($3)}' | awk '{split($0,array,"-")} END{print array[3]}') + 3 )) > /bitnami/zookeeper/data/myid
              env:
              - name: ZOO_SERVER_ID
                value: "3"
              - name: ALLOW_ANONYMOUS_LOGIN
                value: "yes"
              - name: ZOO_SERVERS
                value: zookeeper-0.kafka.svc.slice.local:2888:3888,zookeeper-1.kafka.svc.slice.local:2888:3888,0.0.0.0:2888:3888
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: zookeeper-cluster
      spec:
        selector:
          app: zookeeper-cluster
        ports:
        - name: client
          port: 2181
          targetPort: 2181 
        - name: follower
          port: 2888
          targetPort: 2888 
        - name: leader
          port: 3888
          targetPort: 3888
        clusterIP: None


- name: wait
  ansible.builtin.pause:
    seconds: "{{ worker_deploy_timeout }}"      

########################################################################
#################   Kafka Installation      ########################
#######################################################################


- name: installing kafka on worker 1
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.0.kubeconfig }}"
    # src: files/kafka/kafka-cluster-1.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      #kind: Deployment
      metadata:
        name: kafka-0
      spec:
        replicas: 1
        serviceName: kafka-cluster
        selector:
          matchLabels:
              app: kafka-cluster
        template:
          metadata:
            labels:
              app: kafka-cluster
          spec:
            hostname: kafka
            containers:
            - name: kafka
              image: bitnami/kafka:2.6.0  
      #        image: bitnami/kafka:latest
              securityContext:
                runAsUser: 0
              resources:
                requests:
                  memory: "1G"
                  cpu: "1"             
              ports:
              - containerPort: 9092
              env:
              - name: KAFKA_CFG_ZOOKEEPER_CONNECT
                value: zookeeper-0.kafka.svc.slice.local:2181,zookeeper-1.kafka.svc.slice.local:2181,zookeeper-2.kafka.svc.slice.local:2181
      #          value: zookeeper-cluster:2181
              - name: KAFKA_CFG_ADVERTISED_LISTENERS
                value: PLAINTEXT://kafka-cluster-0.kafka.svc.slice.local:9092
              - name: ALLOW_PLAINTEXT_LISTENER
                value: "yes" 
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-cluster
      spec:
        selector:
          app: kafka-cluster
        ports:
        - port: 9092
          targetPort: 9092
        type: NodePort

- name: installing kafka on worker 2
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.1.kubeconfig }}"
    # src: files/kafka/kafka-cluster-2.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      #kind: Deployment
      metadata:
        name: kafka-1
      spec:
        replicas: 1
        serviceName: kafka-cluster
        selector:
          matchLabels:
              app: kafka-cluster
        template:
          metadata:
            labels:
              app: kafka-cluster
          spec:
            hostname: kafka
            containers:
            - name: kafka
              image: bitnami/kafka:2.6.0  
      #        image: bitnami/kafka:latest
              securityContext:
                runAsUser: 0
              resources:
                requests:
                  memory: "1G"
                  cpu: "1"             
              ports:
              - containerPort: 9092
              env:
              - name: KAFKA_CFG_ZOOKEEPER_CONNECT
                value: zookeeper-0.kafka.svc.slice.local:2181,zookeeper-1.kafka.svc.slice.local:2181,zookeeper-2.kafka.svc.slice.local:2181
      #          value: zookeeper-cluster:2181
              - name: KAFKA_CFG_ADVERTISED_LISTENERS
                value: PLAINTEXT://kafka-cluster-1.kafka.svc.slice.local:9092
              - name: ALLOW_PLAINTEXT_LISTENER
                value: "yes" 
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-cluster
      spec:
        selector:
          app: kafka-cluster
        ports:
        - port: 9092
          targetPort: 9092
        type: NodePort

- name: installing kafka on worker 3
  tags:
  - kafka
  kubernetes.core.k8s:
    state: present
    namespace: kafka
    kubeconfig: "{{ clusters.worker.2.kubeconfig }}"
    # src: files/kafka/kafka-cluster-3.yaml
    validate_certs: "{{ validate_certs }}"
    definition: |
      ---
      apiVersion: apps/v1
      kind: StatefulSet
      #kind: Deployment
      metadata:
        name: kafka-2
      spec:
        replicas: 1
        serviceName: kafka-cluster
        selector:
          matchLabels:
              app: kafka-cluster
        template:
          metadata:
            labels:
              app: kafka-cluster
          spec:
            hostname: kafka
            containers:
            - name: kafka
              image: bitnami/kafka:2.6.0  
      #        image: bitnami/kafka:latest
              securityContext:
                runAsUser: 0
              resources:
                requests:
                  memory: "1G"
                  cpu: "1"             
              ports:
              - containerPort: 9092
              env:
              - name: KAFKA_CFG_ZOOKEEPER_CONNECT
                value: zookeeper-0.kafka.svc.slice.local:2181,zookeeper-1.kafka.svc.slice.local:2181,zookeeper-2.kafka.svc.slice.local:2181
      #          value: zookeeper-cluster:2181
              - name: KAFKA_CFG_ADVERTISED_LISTENERS
                value: PLAINTEXT://kafka-cluster-2.kafka.svc.slice.local:9092
              - name: ALLOW_PLAINTEXT_LISTENER
                value: "yes" 
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-cluster
      spec:
        selector:
          app: kafka-cluster
        ports:
        - port: 9092
          targetPort: 9092
        type: NodePort

- name: wait
  ansible.builtin.pause:
    seconds: "{{ worker_deploy_timeout }}"   


- debug:
    msg:     
    - '###################################################################'
    - '#               Kafka zookeeper Testing                           #'
    - '###################################################################'

- shell: "kubectl --kubeconfig={{ item }} run kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.5.0-debian-11-r1 --namespace kafka --command -- sleep infinity "
  name: Creating kafka client pod on all worker clusters
  no_log: true
  tags:
  - kafka-test
  loop:
     - "{{ clusters.worker.0.kubeconfig }}"
     - "{{ clusters.worker.1.kubeconfig }}"
     - "{{ clusters.worker.2.kubeconfig }}"
  ignore_errors: yes

- name: wait
  ansible.builtin.pause:
    seconds: "{{ chart_deploy_timeout }}"


- name: To produce the message from kafka-client pod from worker-1
  tags:
  - kafka-test
  debug: 
    msg:
    - To exec the kafka client pod use below command 
    - 'kubectl exec --tty -i kafka-client --namespace kafka -- bash'
    - Then inside that pod shell use below command to produce the message
    - 'kafka-console-producer.sh --broker-list kafka-cluster-0.kafka.svc.slice.local:9092 --topic test'  
    - type your message press enter

- name: To Consumed the message on kafka-client pod on worker-2
  tags:
  - kafka-test
  debug: 
    msg:
    - 'To exec the kafka client pod use below command' 
    - 'kubectl exec --tty -i kafka-client --namespace kafka -- bash'
    - 'Then inside that pod shell use below command to Consumed the message'
    - 'kafka-console-consumer.sh --bootstrap-server kafka-cluster-1.kafka.svc.slice.local:9092 --topic test --from-beginning'  
    - 'Now you can see the message which you sent from worker-1'
